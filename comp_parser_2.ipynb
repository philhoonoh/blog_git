{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c50d1b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:140% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:140% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e787a2b3",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/philhoonoh/blog_git/blob/main/comp_parser_2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View Source</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016331b",
   "metadata": {},
   "source": [
    "# Python argparse Part2)\n",
    "  \n",
    "> usuage of argparse python module\n",
    "\n",
    "    - Attempt1. Making parser arguments precedes over config.json\n",
    "    - Attempt2. Handling multiple parameters for parser argument using json format \n",
    "    - Goal. Handling model parameters in depth and changing default setting using config.json \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809f6dc",
   "metadata": {},
   "source": [
    "## Attempt 1. Making parser argument precedes over config.json  \n",
    "\n",
    "> As discussed in the `argparse Part1)`, we want to set defaults using config.json and updating with its parameters from command line input which is more common than vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572a3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2659ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60a0260c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--model_dir'], dest='model_dir', nargs=None, const=None, default='./model', type=<class 'str'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MYDICT = {'key': 'value'}\n",
    "\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--seed', type=int, default=42, help='random seed (default: 42)')\n",
    "parser.add_argument('--epochs', type=int, default=1, help='number of epochs to train (default: 1)')\n",
    "parser.add_argument('--dataset', type=str, default='MaskBaseDataset', help='dataset augmentation type (default: MaskBaseDataset)')\n",
    "parser.add_argument('--augmentation', type=str, default='BaseAugmentation', help='data augmentation type (default: BaseAugmentation)')\n",
    "parser.add_argument(\"--resize\", nargs=\"+\", type=int,default=[128, 96], help='resize size for image when training')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--valid_batch_size', type=int, default=1000, help='input batch size for validing (default: 1000)')\n",
    "parser.add_argument('--model', type=str, default='BaseModel', help='model type (default: BaseModel)')\n",
    "parser.add_argument('--optimizer', type=str, default='SGD', help='optimizer type (default: SGD)')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='learning rate (default: 1e-3)')\n",
    "parser.add_argument('--lr_scheduler', type=str, default='StepLR', help='learning scheduler (default: StepLR)')\n",
    "parser.add_argument('--val_ratio', type=float, default=0.2, help='ratio for validaton (default: 0.2)')\n",
    "parser.add_argument('--criterion', type=str, default='cross_entropy', help='criterion type (default: cross_entropy)')\n",
    "parser.add_argument('--lr_decay_step', type=int, default=20, help='learning rate scheduler deacy step (default: 20)')\n",
    "parser.add_argument('--log_interval', type=int, default=20, help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--name', default='exp', help='model save at {SM_MODEL_DIR}/{name}')\n",
    "parser.add_argument('--config', default='./model_config_custom.json', help='config.json file')\n",
    "parser.add_argument('--early_stopping', type=int, default=5, help='early stopping on validation f-score')\n",
    "parser.add_argument('--lr_sch_params', type=json.loads, default = MYDICT)\n",
    "parser.add_argument('--data_dir', type=str, default='/opt/ml/input/data/train/images')\n",
    "parser.add_argument('--model_dir', type=str, default='./model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8edbbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file):\n",
    "    with open(file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdfb883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'augmentation': 'BaseAugmentation',\n",
      " 'batch_size': 64,\n",
      " 'config': './model_config_custom.json',\n",
      " 'criterion': 'cross_entropy',\n",
      " 'data_dir': '/opt/ml/input/data/train/images',\n",
      " 'dataset': 'MaskBaseDataset',\n",
      " 'early_stopping': 5,\n",
      " 'epochs': 1,\n",
      " 'log_interval': 20,\n",
      " 'lr': 0.001,\n",
      " 'lr_decay_step': 20,\n",
      " 'lr_sch_params': {'key': 'value'},\n",
      " 'lr_scheduler': 'StepLR',\n",
      " 'model': 'BaseModel',\n",
      " 'model_dir': './model',\n",
      " 'name': 'exp',\n",
      " 'optimizer': 'SGD',\n",
      " 'resize': [128, 96],\n",
      " 'seed': 42,\n",
      " 'val_ratio': 0.2,\n",
      " 'valid_batch_size': 1000}\n"
     ]
    }
   ],
   "source": [
    "# checking parser arguments\n",
    "args = parser.parse_args([])\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb143f",
   "metadata": {},
   "source": [
    "> as described in the `parser.add_argument` above, all arguments are now set to script definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ebacb",
   "metadata": {},
   "source": [
    "### model_config_custom.json \n",
    "\n",
    "```python  \n",
    "{\n",
    "    \"train\" : {\n",
    "        \"seed\": 42,\n",
    "        \"epochs\": 50,\n",
    "        \"dataset\": \"MaskSplitByProfileDataset\",\n",
    "        \"augmentation\": \"BasicAugmentation2\",\n",
    "        \"resize\": [224,224],\n",
    "        \"batch_size\": 64,\n",
    "        \"valid_batch_size\": 128,\n",
    "        \"model\": \"EfficientNet\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lr\": 1e-4,\n",
    "        \"lr_scheduler\": \"MultiStepLR\",\n",
    "        \"lr_sch_params\" : {\n",
    "            \"milestones\" : [2,4,6,8],\n",
    "            \"gamma\" : 0.5\n",
    "         },\n",
    "        \"val_ratio\": 0.2,\n",
    "        \"criterion\": \"cross_entropy\",\n",
    "        \"lr_decay_step\": 20,\n",
    "        \"log_interval\": 20,\n",
    "        \"data_dir\": \"/opt/ml/input/data/train/images\",\n",
    "        \"model_dir\": \"./model\",\n",
    "        \"early_stopping\" : 5 \n",
    "    },\n",
    "    \"valid\" : {\n",
    "        \"seed\": 42,\n",
    "        \"batch_size\": 500,\n",
    "        \"resize\": [224,224],\n",
    "        \"model\": \"EfficientNet\",\n",
    "        \"dataset\": \"MaskSplitByProfileDataset\",\n",
    "        \"augmentation\": \"CustomAugmentation\",\n",
    "        \"data_dir\": \"/opt/ml/input/data/train/images\",\n",
    "        \"model_path\": \"\",\n",
    "        \"output_path\": \"\"\n",
    "    },\n",
    "     \"inference\" : {\n",
    "        \"batch_size\": 500,\n",
    "        \"dataset\": \"BasicTestDataset2\",\n",
    "        \"resize\": [224,224],\n",
    "        \"model\": \"EfficientNet\",\n",
    "        \"data_dir\": \"/opt/ml/input/data/eval\",\n",
    "        \"model_path\": \"./model/exp51/\",\n",
    "        \"output_path\": \"./model/exp51\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "230bdbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'augmentation': 'BasicAugmentation2',\n",
      " 'batch_size': 64,\n",
      " 'criterion': 'cross_entropy',\n",
      " 'data_dir': '/opt/ml/input/data/train/images',\n",
      " 'dataset': 'MaskSplitByProfileDataset',\n",
      " 'early_stopping': 5,\n",
      " 'epochs': 50,\n",
      " 'log_interval': 20,\n",
      " 'lr': 0.0001,\n",
      " 'lr_decay_step': 20,\n",
      " 'lr_sch_params': {'gamma': 0.5, 'milestones': [2, 4, 6, 8]},\n",
      " 'lr_scheduler': 'MultiStepLR',\n",
      " 'model': 'EfficientNet',\n",
      " 'model_dir': './model',\n",
      " 'optimizer': 'Adam',\n",
      " 'resize': [224, 224],\n",
      " 'seed': 42,\n",
      " 'val_ratio': 0.2,\n",
      " 'valid_batch_size': 128}\n",
      "<class 'argparse.ArgumentParser'>\n"
     ]
    }
   ],
   "source": [
    "config = read_json(args.config)\n",
    "parser.set_defaults(**config['train'])\n",
    "pprint(parser._defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd48a70",
   "metadata": {},
   "source": [
    "> parser class is now set to defaults according to `model_config_custom.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26442dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'augmentation': 'BasicAugmentation2',\n",
      " 'batch_size': 64,\n",
      " 'config': './model_config_custom.json',\n",
      " 'criterion': 'cross_entropy',\n",
      " 'data_dir': '/opt/ml/input/data/train/images',\n",
      " 'dataset': 'MaskSplitByProfileDataset',\n",
      " 'early_stopping': 5,\n",
      " 'epochs': 50,\n",
      " 'log_interval': 20,\n",
      " 'lr': 0.0001,\n",
      " 'lr_decay_step': 20,\n",
      " 'lr_sch_params': {'gamma': 0.5, 'milestones': [2, 4, 6, 8]},\n",
      " 'lr_scheduler': 'MultiStepLR',\n",
      " 'model': 'ConvNextLIn22Custom',\n",
      " 'model_dir': './model',\n",
      " 'name': 'exp',\n",
      " 'optimizer': 'Adam',\n",
      " 'resize': [224, 224],\n",
      " 'seed': 42,\n",
      " 'val_ratio': 0.2,\n",
      " 'valid_batch_size': 128}\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(['--model', 'ConvNextLIn22Custom'])\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f29f9",
   "metadata": {},
   "source": [
    "> parser changed the model argument (EfficientNet -> ConvNextLIn22Custom) from in-line command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2120a15b",
   "metadata": {},
   "source": [
    "### Recap on attempt1 \n",
    "\n",
    "```python\n",
    "args = parser.parse_args(['--model', 'ConvNextLIn22Custom'])\n",
    "def read_json(file):\n",
    "    with open(file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "config = read_json(args.config)\n",
    "```\n",
    "> Here parser do checkout out any command line arguments; however, it only uses either default config.json path or specified config.json path from command line.  \n",
    "\n",
    "```python\n",
    "parser.set_defaults(**config['train'])\n",
    "```\n",
    "> Second, setting parser defaults with the given config.json.   \n",
    "\n",
    "```python\n",
    "args = parser.parse_args(['--model', 'ConvNextLIn22Custom'])\n",
    "```\n",
    "> Third, overwriting parser with command line arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b21b69",
   "metadata": {},
   "source": [
    "```zsh\n",
    "$ python train.py --model ConvNextLIn22Custom\n",
    "# setting parameters with default `./model_config_custom.json` but only changing model parameter\n",
    "```\n",
    "\n",
    "```zsh\n",
    "$ python train.py --config ./model_config_other.json\n",
    "# setting parameters with ./model_config_other.json\n",
    "```\n",
    "\n",
    "```zsh\n",
    "$ python train.py --config ./model_config_other.json --model ConvNextLIn22Custom\n",
    "# setting parameters with './model_config_other.json' and changing model parameter\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e62e0",
   "metadata": {},
   "source": [
    "### Conclusion on attempt1\n",
    "\n",
    "> config.json must contain all the parameters required for arguments   \n",
    "> Personally prefer changing config.json over using command-line arguments  \n",
    "> For each time of training, saving config.json along with model output would be a good practice to start tracking down parameter searching (Of course there are tons of better ways of doing this such as `wandb`, `mlflow` and etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5e1be",
   "metadata": {},
   "source": [
    "## Attempt 2. Handling multiple parameters for parser argument using json format\n",
    "\n",
    "    > Here we are going to create argumnet for `learning scheduler` class and its own `parameters`\n",
    "    > Unsurprisingly, this can be done using `json`, `dictionary`, and `**` unpacking. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ccfc0",
   "metadata": {},
   "source": [
    "In Pytorch, there are many lr_scheduler such as `CosineAnnealingLR`, `MultiStepLR`, `CyclicLR` and so on.  \n",
    "They do have their own parameters.   \n",
    "  \n",
    "`CosineAnnealingLR` :  \"T_max\", \"eta_min\"  \n",
    "`MultiStepLR` : \"milestones\", \"gamma\"  \n",
    "`CyclicLR` : \"cycle_momentum\", \"max_lr\", \"base_lr\", \"step_size_up\" : 50, \"step_size_down\", \"mode\" : \"triangular\" \n",
    "\n",
    "Let's denote those parameters as `lr_sch_params`. So we want to pass `lr_sch_params` from the __command-line__ or __config.json__.\n",
    "\n",
    "Luckily, for config file, we can apply same format since config.json is converted into dictionary class.   \n",
    "That is, for each `lr_sscheduler` adding `lr_sch_params` with its arguments\n",
    "\n",
    "```python\n",
    "\n",
    "\"lr_scheduler\": \"MultiStepLR\",\n",
    "\"lr_sch_params\" : {\n",
    "    \"milestones\" : [2,4,6,8],\n",
    "    \"gamma\" : 0.5\n",
    " },\n",
    "\n",
    "\n",
    "\"lr_scheduler\": \"CyclicLR\",\n",
    "\"lr_sch_params\" : {\n",
    "    \"cycle_momentum\" : false,\n",
    "    \"max_lr\" : 0.1,\n",
    "    \"base_lr\" : 0.001,\n",
    "    \"step_size_up\" : 50,\n",
    "    \"step_size_down\" : 100,\n",
    "    \"mode\" : \"triangular\"\n",
    " },    \n",
    "\n",
    "\n",
    "\"lr_scheduler\": \"CosineAnnealingLR\",\n",
    "\"lr_sch_params\" : {\n",
    "    \"T_max\" : 10,\n",
    "    \"eta_min\" : 0\n",
    " },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5a17b",
   "metadata": {},
   "source": [
    "Problem arises from __command-line__ (at least for me). \n",
    "As always, answer can be found on [stackover flow](https://stackoverflow.com/questions/18608812/accepting-a-dictionary-as-an-argument-with-argparse-and-python)  \n",
    "Using json.loads on type  \n",
    "\n",
    "```python\n",
    "MYDICT = {'key': 'value'} # for default mean nothing\n",
    "parser.add_argument('--lr_sch_params', type=json.loads, default = MYDICT)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683987ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d5abdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--model_dir'], dest='model_dir', nargs=None, const=None, default='./model', type=<class 'str'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MYDICT = {'key': 'value'}\n",
    "\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--seed', type=int, default=42, help='random seed (default: 42)')\n",
    "parser.add_argument('--epochs', type=int, default=1, help='number of epochs to train (default: 1)')\n",
    "parser.add_argument('--dataset', type=str, default='MaskBaseDataset', help='dataset augmentation type (default: MaskBaseDataset)')\n",
    "parser.add_argument('--augmentation', type=str, default='BaseAugmentation', help='data augmentation type (default: BaseAugmentation)')\n",
    "parser.add_argument(\"--resize\", nargs=\"+\", type=int,default=[128, 96], help='resize size for image when training')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--valid_batch_size', type=int, default=1000, help='input batch size for validing (default: 1000)')\n",
    "parser.add_argument('--model', type=str, default='BaseModel', help='model type (default: BaseModel)')\n",
    "parser.add_argument('--optimizer', type=str, default='SGD', help='optimizer type (default: SGD)')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='learning rate (default: 1e-3)')\n",
    "parser.add_argument('--lr_scheduler', type=str, default='StepLR', help='learning scheduler (default: StepLR)')\n",
    "parser.add_argument('--val_ratio', type=float, default=0.2, help='ratio for validaton (default: 0.2)')\n",
    "parser.add_argument('--criterion', type=str, default='cross_entropy', help='criterion type (default: cross_entropy)')\n",
    "parser.add_argument('--lr_decay_step', type=int, default=20, help='learning rate scheduler deacy step (default: 20)')\n",
    "parser.add_argument('--log_interval', type=int, default=20, help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--name', default='exp', help='model save at {SM_MODEL_DIR}/{name}')\n",
    "parser.add_argument('--config', default='./model_config_custom.json', help='config.json file')\n",
    "parser.add_argument('--early_stopping', type=int, default=5, help='early stopping on validation f-score')\n",
    "parser.add_argument('--lr_sch_params', type=json.loads, default = MYDICT)\n",
    "parser.add_argument('--data_dir', type=str, default='/opt/ml/input/data/train/images')\n",
    "parser.add_argument('--model_dir', type=str, default='./model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb652b11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'augmentation': 'BasicAugmentation2',\n",
      " 'batch_size': 64,\n",
      " 'config': './model_config_custom.json',\n",
      " 'criterion': 'cross_entropy',\n",
      " 'data_dir': '/opt/ml/input/data/train/images',\n",
      " 'dataset': 'MaskSplitByProfileDataset',\n",
      " 'early_stopping': 5,\n",
      " 'epochs': 50,\n",
      " 'log_interval': 20,\n",
      " 'lr': 0.0001,\n",
      " 'lr_decay_step': 20,\n",
      " 'lr_sch_params': {'T_max': 10, 'eta_min': 0},\n",
      " 'lr_scheduler': 'CosineAnnealingLR',\n",
      " 'model': 'EfficientNet',\n",
      " 'model_dir': './model',\n",
      " 'name': 'exp',\n",
      " 'optimizer': 'Adam',\n",
      " 'resize': [224, 224],\n",
      " 'seed': 42,\n",
      " 'val_ratio': 0.2,\n",
      " 'valid_batch_size': 128}\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(['--lr_scheduler', 'CosineAnnealingLR', '--lr_sch_params', '{ \"T_max\" : 10, \"eta_min\" : 0}'])\n",
    "config = read_json(args.config)\n",
    "parser.set_defaults(**config['train'])\n",
    "args = parser.parse_args(['--lr_scheduler', 'CosineAnnealingLR', '--lr_sch_params', '{ \"T_max\" : 10, \"eta_min\" : 0}'])\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd017646",
   "metadata": {},
   "source": [
    "> Lastly these paraemters can be passed to lr_scheduler class using unpacking. \n",
    "\n",
    "```python\n",
    "sch_module = getattr(import_module('torch.optim.lr_scheduler'), args.lr_scheduler)\n",
    "scheduler = sch_module(\n",
    "    optimizer,\n",
    "    **args.lr_sch_params\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa17df4",
   "metadata": {},
   "source": [
    "### Recap on attempt2 \n",
    "\n",
    "> above example is equivalent to command-line below\n",
    "\n",
    "```zsh\n",
    "$ python train.py --lr_scheduler CosineAnnealingLR --lr_sch_params '{\"T_max\" : 10, \"eta_min\" : 0}'\n",
    "# setting parameters with default `./model_config_custom.json` but changing lr_scheduler & lr_sch_params\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132f445",
   "metadata": {},
   "source": [
    "### Conclusion on attempt2\n",
    "\n",
    "> json.load is a nice hack!  \n",
    "\n",
    "> __Caution__ In windows, pass -m {\\\\\\\"key1\\\\\\\":\\\\\\\"value1\\\\\\\"} in the terminal.(adding backslash \\ in front of \", without spaces in the whole {} string) by [Johnson Lai](https://stackoverflow.com/users/4767011/johnson-lai)  \n",
    "\n",
    "> With this, most of configurations can be done easily just changing `config.json`.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_transformers",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
