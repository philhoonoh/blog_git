{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef4c7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:140% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:140% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb972cf",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/philhoonoh/blog_git/blob/main/comp_models_1.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View Source</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1040efc7",
   "metadata": {},
   "source": [
    "# PyTorch Model - timm library, torchvision.models part1)\n",
    "  \n",
    "> Basic usage of timm and torchvision.model libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc909392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7cf207",
   "metadata": {},
   "source": [
    "## Timm Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32adb51",
   "metadata": {},
   "source": [
    "#### Select Model\n",
    "```python\n",
    "timm.list_models('model name or regular exp', pretrained = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19852a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convnext_base',\n",
       " 'convnext_base_384_in22ft1k',\n",
       " 'convnext_base_in22ft1k',\n",
       " 'convnext_base_in22k',\n",
       " 'convnext_large',\n",
       " 'convnext_large_384_in22ft1k',\n",
       " 'convnext_large_in22ft1k',\n",
       " 'convnext_large_in22k',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'convnext_xlarge_384_in22ft1k',\n",
       " 'convnext_xlarge_in22ft1k',\n",
       " 'convnext_xlarge_in22k']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_densenet_models = timm.list_models('*convnext*', pretrained = True)\n",
    "all_densenet_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e1092",
   "metadata": {},
   "source": [
    "#### Check Model Architecture\n",
    "\n",
    "```python\n",
    "model = timm.create_model('convnext_small', pretrained=True)\n",
    "print(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529d5a24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = timm.create_model('convnext_small', pretrained=True)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95cfee4",
   "metadata": {},
   "source": [
    "#### Change the last layer of the Model\n",
    "```python\n",
    "model = timm.create_model('convnext_small', pretrained=True, num_classes = 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a5adfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('convnext_small', pretrained=True, num_classes = 100)\n",
    "# print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebe054",
   "metadata": {},
   "source": [
    "####  Testing \n",
    "\n",
    "```python\n",
    "model = timm.create_model('convnext_small', pretrained=True, num_classes = 100)\n",
    "model.eval()\n",
    "result = model(torch.randn(1,3,224,224))\n",
    "print(result)\n",
    "print(result.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d289eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3662e-02, -7.4553e-02,  8.9252e-02,  4.5729e-02,  2.6010e-02,\n",
      "          2.4094e-02, -1.8512e-02,  4.8652e-02, -7.9148e-03, -5.5573e-02,\n",
      "         -1.3695e-02, -1.6126e-02, -3.9224e-02, -6.2003e-02,  2.6231e-02,\n",
      "          6.7386e-02,  2.3819e-03,  1.4252e-02, -1.3974e-01, -1.2448e-01,\n",
      "         -2.1656e-02,  7.6230e-02, -2.3944e-02, -3.5192e-02, -4.6354e-02,\n",
      "         -1.3579e-02, -7.5800e-03,  3.2146e-02, -6.9150e-03,  5.8752e-02,\n",
      "         -3.9741e-02,  6.0090e-02,  6.0075e-02,  2.0687e-02, -9.0599e-03,\n",
      "          3.8749e-02,  5.6696e-02,  1.9821e-02, -9.1513e-03, -3.3444e-02,\n",
      "          7.5849e-02, -2.3506e-02, -1.3826e-02,  8.0334e-03,  4.5473e-02,\n",
      "         -9.8301e-02, -2.3908e-02, -1.5019e-02,  2.9877e-03,  1.1585e-01,\n",
      "         -3.2192e-02, -3.4125e-02, -3.4668e-02, -1.4774e-02, -5.2207e-03,\n",
      "         -1.1825e-02,  4.9037e-02,  4.8691e-02, -8.1260e-02, -5.8519e-02,\n",
      "         -3.0105e-03, -2.5320e-02,  6.2025e-02,  6.8373e-02,  4.6517e-02,\n",
      "          2.2243e-02, -1.0411e-01,  1.9704e-02,  2.4856e-02,  2.6043e-02,\n",
      "          1.6694e-02, -1.6419e-02,  8.3229e-03,  5.3576e-02,  4.2734e-02,\n",
      "          2.7652e-02, -3.8295e-03, -6.7452e-02,  1.3147e-02,  6.1153e-02,\n",
      "         -7.3545e-05,  3.8735e-02,  2.7421e-02,  9.3548e-03,  7.2735e-02,\n",
      "          1.4300e-02, -2.1304e-02,  6.3397e-02, -5.5538e-04,  5.4154e-02,\n",
      "          2.2753e-02, -9.2972e-02,  4.7925e-03, -1.6918e-02, -5.1844e-03,\n",
      "         -7.6301e-03, -5.7778e-02,  6.5509e-02,  2.0717e-03, -6.6231e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('convnext_small', pretrained=True, num_classes = 100)\n",
    "model.eval()\n",
    "result = model(torch.randn(1,3,224,224))\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ba258",
   "metadata": {},
   "source": [
    "## Torchvision models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d304e6a",
   "metadata": {},
   "source": [
    "#### Basic Template for Model\n",
    "\n",
    "> applicable any libraries based on pytorch\n",
    "\n",
    "```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf63b61",
   "metadata": {},
   "source": [
    "#### Select Model\n",
    "\n",
    "> Check out the list of models on https://pytorch.org/vision/master/models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa21c3",
   "metadata": {},
   "source": [
    "#### Check Model Architecture\n",
    " \n",
    "```python\n",
    "model = models.densenet161(pretrained = True)\n",
    "print(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f0b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.densenet161(pretrained = True)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c49b4",
   "metadata": {},
   "source": [
    "#### Change the last layer of the Model\n",
    "\n",
    "> Needs to manually check out the name of last layer : usually either 'fc' or 'classifier'  \n",
    "\n",
    "```python\n",
    "num_classes = 100\n",
    "model.classifier = nn.Linear(in_features = model.classifier.in_features,\n",
    "                                      out_features = num_classes, bias = True)\n",
    "print(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a006b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "model.classifier = nn.Linear(in_features = model.classifier.in_features,\n",
    "                                      out_features = num_classes, bias = True)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24969d",
   "metadata": {},
   "source": [
    "####  Testing \n",
    "\n",
    "```python\n",
    "model = models.densenet161(pretrained = True))\n",
    "num_classes = 100\n",
    "model.classifier = nn.Linear(in_features = model.classifier.in_features,\n",
    "                                      out_features = num_classes, bias = True)\n",
    "model.eval()\n",
    "result = model(torch.randn(1,3,224,224))\n",
    "print(result)\n",
    "print(result.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8ed94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.2901e-01, -9.3326e-02,  3.0764e-01, -3.9876e-01, -1.0776e+00,\n",
      "         -2.5174e-01,  5.5095e-01, -8.0655e-01, -3.6235e-01, -1.4524e-03,\n",
      "          2.8701e-01,  7.0176e-02,  4.9840e-01, -1.6914e-01,  3.5931e-01,\n",
      "          3.6778e-01,  2.5416e-02, -5.3275e-01,  2.3207e-02,  3.5605e-01,\n",
      "         -1.0407e-01, -4.7926e-01, -1.5813e-01,  1.4952e-01,  2.1654e-01,\n",
      "          6.2253e-01,  2.3181e-02,  4.4012e-02, -2.1253e-01,  2.0279e-01,\n",
      "         -7.2999e-01,  1.1075e-01, -1.7469e-01, -4.1017e-02, -6.8784e-01,\n",
      "          1.8264e-01,  1.7086e-01,  3.7765e-01,  3.7762e-01, -2.8333e-01,\n",
      "         -4.6434e-01, -2.4231e-01,  2.4459e-01,  3.7019e-01,  1.9268e-01,\n",
      "         -4.3335e-01,  1.6836e-01,  9.9497e-02,  2.1062e-02, -6.2166e-01,\n",
      "         -7.4108e-01, -4.9581e-02, -1.1433e-01, -4.2303e-01, -1.9275e-01,\n",
      "          1.7361e-01, -2.5386e-01, -5.6183e-01, -2.3228e-01,  1.6434e-01,\n",
      "         -2.9348e-01, -8.9558e-02, -7.4834e-04,  2.0683e-01,  2.4919e-01,\n",
      "          2.7467e-01,  3.0369e-01,  4.1775e-01,  8.4553e-02, -1.9007e-01,\n",
      "          2.5639e-01, -2.2361e-01, -3.5536e-01,  1.9711e-01,  2.3427e-01,\n",
      "          4.0014e-02,  3.5994e-01, -1.0370e-01,  1.8815e-01, -2.1634e-01,\n",
      "          2.9653e-01,  7.6269e-02,  6.4522e-01, -1.8169e-01, -2.1026e-01,\n",
      "         -4.0313e-02, -6.4192e-01,  5.2914e-01,  4.8153e-01, -3.6993e-01,\n",
      "         -1.3434e-01, -5.8851e-01, -1.0627e-01,  1.0534e-01, -1.2358e-01,\n",
      "          4.1891e-01,  6.1412e-01, -3.2129e-01, -1.1038e-01,  1.9823e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "model = models.densenet161(pretrained = True)\n",
    "num_classes = 100\n",
    "model.classifier = nn.Linear(in_features = model.classifier.in_features,\n",
    "                                      out_features = num_classes, bias = True)\n",
    "model.eval()\n",
    "result = model(torch.randn(1,3,224,224))\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_transformers",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
